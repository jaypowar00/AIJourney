# ğŸš€ AIJourney â€” Quick Start Guide

## One-Minute Setup

### Prerequisites
- âœ… Python 3.9+
- âœ… Ollama running with a model (`ollama pull deepseek-r1`)

### Step 1: Install Dependencies (PowerShell)

```powershell
cd c:\Users\jaypo\Documents\Projects\AI Learning\AIJourney
pip install fastapi uvicorn streamlit requests sentence-transformers ollama langchain langchain-community langchain-huggingface chromadb
```

### Step 2: Start Backend (Terminal 1)

```powershell
uvicorn ai_agent.backend.server:app --reload
```

**Expected output:**
```
Uvicorn running on http://127.0.0.1:8000
```

### Step 3: Start Frontend (Terminal 2)

```powershell
cd c:\Users\jaypo\Documents\Projects\AI Learning\AIJourney
streamlit run ai_agent/frontend/streamlit_app.py
```

**Expected output:**
```
You can now view your Streamlit app in your browser.
Local URL: http://localhost:8501
```

### Step 4: Use the App

1. **Sidebar**: Add documents
2. **Main Area**: Ask questions
3. **Results**: View answers with sources & confidence

---

## What Happens When You Ask a Question?

```
ğŸ“ Your Query
    â†“
ğŸ” Search local documents (semantic similarity)
    â†“
ğŸ“Š Check confidence score
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Confidence High â”‚ â†’ Use local context only
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â””â”€â†’ Low? â†’ ğŸŒ Search web for more info
                â†“
           ğŸ¤– Combine all context
                â†“
           ğŸ’¬ Ollama generates answer
                â†“
           ğŸ“¤ Return answer + metadata
```

---

## Key Features

| Feature | What It Does |
|---------|-------------|
| ğŸ’¾ **Document Storage** | Save text docs locally in CSV |
| ğŸ” **Semantic Search** | Find relevant docs using embeddings |
| ğŸ¤– **AI Agent** | Ask questions; agent retrieves context + calls LLM |
| ğŸŒ **Web Search Fallback** | If confidence is low, searches the web |
| ğŸ“Š **Confidence Score** | Shows how sure the agent is (0-1) |
| ğŸ¨ **Professional UI** | Clean, modern Streamlit interface |

---

## Troubleshooting

| Problem | Solution |
|---------|----------|
| Backend won't start | Check if model files exist in `ai_agent/models/sentence-transformer/` |
| "Connection refused" | Make sure backend is running on port 8000 |
| Ollama timeout | Run `ollama serve` in separate terminal |
| "No results found" | Add documents first using the sidebar |

---

## API Documentation

### View Interactive Docs
- **Swagger**: http://127.0.0.1:8000/docs
- **ReDoc**: http://127.0.0.1:8000/redoc

### Example cURL Commands

**Add a document:**
```powershell
curl -X POST http://127.0.0.1:8000/documents `
  -H "Content-Type: application/json" `
  -d '{"content": "Python is great for AI"}'
```

**Ask the agent:**
```powershell
curl -X POST http://127.0.0.1:8000/agent/answer `
  -H "Content-Type: application/json" `
  -d '{"query": "What is Python?", "use_web_search": true}'
```

---

## File Overview

```
ai_agent/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.py ............. FastAPI + Agent Logic
â”‚   â”œâ”€â”€ storage.py ............ CSV Storage
â”‚   â””â”€â”€ data/docs.csv ......... Document Database
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ streamlit_app.py ...... Professional UI
â”œâ”€â”€ agent_demo.py ............ Original demo
â”œâ”€â”€ models/sentence-transformer/ Local embeddings
â””â”€â”€ README_SETUP.md .......... Full documentation
```

---

## Configuration

Edit `ai_agent/backend/server.py` to customize:

```python
CHUNK_SIZE = 800           # Chunk size for splitting
CHUNK_OVERLAP = 100        # Chunk overlap
TOP_K = 3                  # Results to retrieve
CONFIDENCE_THRESHOLD = 0.7 # Threshold for web search
LLM_MODEL = "deepseek-r1"  # Ollama model name
```

---

## Next Steps

- ğŸ“š Add your own documents
- â“ Ask questions about them
- ğŸ”§ Customize the LLM model or chunking
- ğŸ—„ï¸ Later: Switch CSV â†’ MongoDB/PostgreSQL
- ğŸŒ Later: Add real SerpAPI key for web search

---

**You're all set! ğŸ‰**

Visit http://localhost:8501 and start using AIJourney!
