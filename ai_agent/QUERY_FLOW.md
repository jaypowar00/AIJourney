# ðŸ”„ Complete Query Flow

```
USER ENTERS QUESTION IN STREAMLIT
          â†“
    (POST /agent/answer)
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BACKEND PROCESSES:                      â”‚
â”‚                                         â”‚
â”‚ 1. Load all docs from CSV               â”‚
â”‚ 2. Build vector DB (Chroma + ST model)  â”‚
â”‚ 3. Semantic search (retrieve TOP_K=3)   â”‚
â”‚ 4. Compute confidence score             â”‚
â”‚                                         â”‚
â”‚ IF confidence >= 0.7:                   â”‚
â”‚   â†’ Use local context only              â”‚
â”‚                                         â”‚
â”‚ ELSE (confidence < 0.7):                â”‚
â”‚   â†’ Trigger web search                  â”‚
â”‚   â†’ Check semantic relevance            â”‚
â”‚   â†’ Combine local + web results         â”‚
â”‚                                         â”‚
â”‚ 5. Generate answer using Ollama LLM     â”‚
â”‚ 6. Return: {answer, confidence,         â”‚
â”‚           used_web_search, chunks}      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
    STREAMLIT DISPLAYS:
    - Answer in "Answer" tab
    - Sources in "Sources" tab
    - Metrics in "Details" tab
```

---

## ðŸš€ How to Run

### Terminal 1: Backend
```powershell
uvicorn ai_agent.backend.server:app --reload
```

### Terminal 2: Frontend
```powershell
streamlit run ai_agent/frontend/streamlit_app.py
```

### Access
- **UI**: http://localhost:8501
- **API Docs**: http://127.0.0.1:8000/docs

---

## ðŸ“Š Configuration Constants

Edit in `ai_agent/backend/server.py`:

```python
CHUNK_SIZE = 800           # How big each doc chunk is
CHUNK_OVERLAP = 100        # Overlap between chunks
TOP_K = 3                  # Number of local docs to retrieve
CONFIDENCE_THRESHOLD = 0.7 # Trigger for web search (0-1)
LLM_MODEL = "deepseek-r1"  # Ollama model (change to gpt2, llama2, etc.)
```

---

## ðŸ”§ Customization Examples

### Change LLM Model
```python
LLM_MODEL = "llama2"  # or "mistral", "neural-chat", etc.
# Then: ollama pull mistral
```

### Increase Web Search Trigger Sensitivity
```python
CONFIDENCE_THRESHOLD = 0.5  # Lower = more web searches
```

### Adjust Document Chunking
```python
CHUNK_SIZE = 1200          # Larger chunks = more context
CHUNK_OVERLAP = 50         # Less overlap = faster processing
```

### Switch to Different Embeddings Model
Edit `backend/server.py`:
```python
MODEL_PATH = "sentence-transformers/all-mpnet-base-v2"  # Different model
```

---

## ðŸ“ File Structure

```
ai_agent/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py ................... Package marker
â”‚   â”œâ”€â”€ server.py ..................... FastAPI app + Agent logic (380 lines)
â”‚   â”œâ”€â”€ storage.py .................... CSV storage functions (50 lines)
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ docs.csv .................. Document database (auto-created)
â”‚   â””â”€â”€ models/sentence-transformer/
â”‚       â””â”€â”€ [model files already exist]
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ __init__.py ................... Package marker
â”‚   â””â”€â”€ streamlit_app.py .............. Professional UI (300+ lines)
â”‚
â”œâ”€â”€ agent_demo.py ..................... Original agent demo
â”œâ”€â”€ requirements-agent.txt ............ Dependencies
â””â”€â”€ README.md ......................... Original readme
```

---

## ðŸŽ¨ UI Highlights

### Professional Features
- âœ… Gradient theme (blue professional palette)
- âœ… Session state management (persistent conversation)
- âœ… Responsive layout (wide mode)
- âœ… Tabbed results display
- âœ… Real-time metrics (confidence, sources used)
- âœ… Document preview in sidebar
- âœ… Conversation history with timestamps
- âœ… Clear error handling with user-friendly messages
- âœ… Loading spinners for better UX

### Color Scheme
- Primary: `#1e3c72` (dark blue)
- Secondary: `#2c5aa0` (medium blue)
- Accent: `#4a90e2` (light blue)
- Background: Light gradient

---

## ðŸ” Agent Intelligence

The agent is **smart** about what it does:

### Scenario 1: High Confidence
```
Q: "What is Python?" (with Python docs stored)
â†’ Local confidence: 0.85 (HIGH)
â†’ Uses only local context
â†’ Direct answer
```

### Scenario 2: Low Confidence
```
Q: "Latest AI news?" (no specific docs)
â†’ Local confidence: 0.45 (LOW)
â†’ Triggers web search
â†’ Combines web + local
â†’ Enhanced answer
```

### Scenario 3: Web Not Relevant
```
Q: "Custom internal process?" (with web search)
â†’ Local confidence: 0.35 (LOW)
â†’ Web search triggered
â†’ Relevance check fails (web results off-topic)
â†’ Returns message: "Not enough reliable information"
```

---

## ðŸš€ Next Steps (Future Enhancements)

### Phase 2: Database
- Replace CSV with MongoDB/PostgreSQL
- Add embeddings caching (speed improvement)

### Phase 3: Advanced Features
- Document upload (PDF, DOCX, TXT)
- Multi-language support
- Fine-tuning on custom data
- User authentication

### Phase 4: Scaling
- Distributed embeddings (FAISS)
- Real-time web search (SerpAPI, Tavily)
- Model serving (vLLM, TGI)

---

## ðŸ“š Documentation

Three levels of documentation provided:

1. **QUICKSTART.md** â€” Get running in 5 minutes
2. **README_SETUP.md** â€” Full setup + API reference
3. **This file** â€” Architecture + customization guide

---

## âœ¨ Key Takeaways

âœ… **Full Stack**: Backend + Frontend + Storage
âœ… **Intelligent**: Confidence-based web search fallback
âœ… **Professional**: Modern UI with conversation history
âœ… **Extensible**: Easy to swap CSV for DB, LLM for LLM
âœ… **Local-First**: All embeddings locally (no API calls)
âœ… **Production-Ready**: Error handling, logging, docs

---

## ðŸŽ‰ You're All Set!

Your AIJourney RAG system is ready to use. Start by:

1. Run both terminals (backend + frontend)
2. Add some documents
3. Ask questions
4. Watch the agent retrieve context and generate answers

For detailed setup, see **QUICKSTART.md**

---

**Built with â¤ï¸ by JayPowar | November 2025**
